I"	<p>Building Systems to Monitor Data and Model Health in Production Systems. Unlike traditional deterministic software, models in production start to degrade as soon as they have been deployed.</p>

<p>Typically these degradations are caused by three broad types of problems:</p>

<ol>
  <li>
    <p>Bugs in the data data pipeline: This can manifest in many different ways e.g. someone changes code upstream and now a particular column could be filled with NaNs.</p>
  </li>
  <li>
    <p>Input distribution shifts: This is caused by changes in real world phenomenon e.g. a bank has a churn prediction model - their competitor has just released a new competitive campaign. This in itself can manifest in distribution shifts as customer behavior changes.</p>
  </li>
  <li>
    <p>Concept drift: This happens when the actual relationship between the input and output changes.</p>
  </li>
</ol>

<p>Traditional testing infrastructure is currently not suitable to handle the dynamic nature of machine learning models.</p>

<iframe width="100%" height="500" src="https://www.youtube.com/embed/WZNEsc7ynxI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<blockquote>
  <p>In this talk, Mohammed Ridwanul, Product Manager at Dessa, will speak about writing tests to monitor machine learning models, creating data contracts and metadata stores for reference checks and building automated systems around model testing to prevent degradation in production.</p>
</blockquote>

:ET